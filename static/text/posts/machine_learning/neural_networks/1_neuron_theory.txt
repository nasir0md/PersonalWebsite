TITLE:Building Blocks of a Neural Network. The Neuron
|
NEXT:2_training_neural_networks
|
PREV:introduction
|
Date:Feb 8, 2017
|
META: tmp
|
<p>
  What is a neural network? Let's begin by dissecting the name itself.
  A network can be though of as a graph and is nothing more than a set of
  vertices or <i>nodes</i> that are connected via edges. An example of a
  network is shown below. 
</p>

<img class='center-image' src='/static/img/directed_acyclic_graph.png'></img>

<p>
    Each of the circles with a number in it corresponds to a node. Note that
    the edges connecting nodes have a one way relation. Say you wanted to get
    from node 7 to node 10. You would start at node 7, go to 11, then 10.
    However, going from node 10 to 7 would be impossible. We will see that
    neural networks are represented in much the same manner as a directed graph
    where each node represents a neuron with edges connecting neurons together.
</p>

<p>
    The other important aspect of our neural network is the neural part. We
    want our network to be able to understand and learn. While this might seem
    far fetched we will see that neural networks use these graphs as a powerful
    structure to define operations to learn desired responses. 
    Similar to the real historical beginnings of
    neural networks let's use inspirations of biology to get started. 
</p>

<p>
    Examine an individual node in our graph. Inspired by the brain we 
    will call this a neuron. An illustration of a neuron is shown below. 
</p>

<img class='center-image' src='/static/img/single-neuron.png'></img>

<p>
  Before we get into the actual intuition of what a neuron is, let's get
  through the math behind one. 
</p>

<p>
    The input of the incoming edge is notated as the scalar \( p \). This edge has 
    a weight of the scalar \( w \). The weight is multiplied by the input to form the value 
    \( wp \). This is then sent into the the summation block which sums the 
    input \( wp \) and the bias \( b \). Notice that the bias has no 
    dependence on external input. Summing these two terms together then 
    gives \( wp + b \) or in the above image \( n \).
</p>

<p>
    The output of the summation \( n \) is then passed through the activation 
    function \( f \). The activation function is just some real valued scalar
    function. This then gives the final output of the neuron 
    \( f(n) = a \). The neuron output can then be calculated as 
    $$ a = f(wp + b) $$
</p>





<p>
  So what is the intuition behind a neuron? We can view the output of this
  neuron as making a <i>decision</i>.  This decision is based on the
  inputs, weights and bias of the neuron. Let's say you are trying to make the
  decision of if you want to go to a party tonight. Let's say we live in a
  world where this decision depends on only two factors: are you tired and is
  your best friend at the party? Note that these factors are simple yes or no
  questions. We can <i>encode</i> yes as \( 1 \) and no as \( 0 \). 
</p>

<p>
  The importance of these two factors will vary a lot from person to person.
  This corresponds to different weight values. For this neuron say that our
  activation function is the simple linear function \( f(x) = x \) where we say
  that any output \( > 0 \) means we should decide to go to the party and any
  output \( < 0 \) means that we should decide to not go. A normal person would
  not want to go to a party while tired. We should then make the weight (\(w_0
  \)) for the are you tired input (\( x_0 \)) be negative. Hopefully the
  opposite is true about your best friend, so the  weight (\(w_1\)) for that input
  (\(x_1\)) would be positive. 
</p>

<p>
  Say that you absolutely hate going out when you are tired and this is far
  more important than if your best friend is at the party. We could make \(
  w_0 = -10 \) and \( w_1 = 1 \) to represent this. If you  are tired you will
  never go out even if your best friend is there \( -10 + 1 < 0 \). However, if
  your best friend is there but you are not tired you would still go out \( 0 +
  1 > 0 \). If you were not tired and your best friend wasn't there you would
  be on the decision boundary and could just choose randomly.
</p>

<p>
  We can now incorperate bias to change our decision boundary. When we had no
  bias in the previous example the decision boundary was 0. If we set the bias
  to be \( 1 \) the decision boundary will be \( 1 \). A more positive decision
  boundary means we are less inclined to go to parties given any inputs. Let's
  change the problem slightly for \( x_1 \) to be the number of your friends
  that are going. If you generally enjoy going to parties your decision neuron
  could have \( b = -2 \) and of course you want your friends to be there and
  do not like going to a party while tired so \( w_0 = -4, w_1 = 1 \). So even
  if you are tired it would only take three of your friends to be there for you
  to want to go to the party. But if \( b = 0 \) it would take five friends if
  you are tired.
</p>

<p>
  In our example we choose the linear action function where our equation took
  the form \( a = w_0 x_0 + w_1 x_1 + b \). While we choose the linear action
  function there are a variety of other activation functions that are employed
  in neurons. Our simple linear linear activation function would look like the
  below for input \( p \) and output \( a \). 
</p>

<img class='center-image' src='/static/img/pure-linear-transform.png'></img>

<p>
  Going back to the decision about the party, say there is another person that
  is trying to predict if you are going to go to the party. The neuron that
  represents this should have a probabilistic output so our same hard rule of if
  the output is greater than \( 1 \) will not apply. We could just take the
  pure value and based on how positive or negative it is determine how certain
  you are to go to the party. However, there is a function called the
  <i>sigmoid</i> function that does a better job of representing these
  probabilistic outputs. Any probability can be represented between 0 and 1.
  The sigmoid function does just this. Below is an image of the sigmoid
  function. 
</p>
<img class='center-image' src='/static/img/sigmoid.png'></img>


<p>
  So very negative outputs will produce values close to 0 and very positive
  outputs will produce outputs close to 1. 
</p>

<p>
  Neural networks are probabilistic systems and therefore functions like the
  sigmoid function are a lot more powerful than just the linear activation
  function. We will see why the sigmoid function and other non-linear functions
  are so powerful in later lessons.
</p>

<p>
  Before moving on let's clean up some of the math behind what we have
  developed with the neuron so far. Say we have the multiple input neuron
  pictured below. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/multiple_input_neuron.png'></img>
<p>
  Each of the inputs to the node can just be represented as a vector to make
  the representation easier. 
  $$ \textbf{p} = \begin{bmatrix}
                    p_1 \\
                    p_2 \\
                    p_3 \\
                    \vdots \\
                    p_R
                  \end{bmatrix}$$
  Know that bold face represents a vector. 
</p>
<p>
  Likewise, we can also formulate the list of weights for each input value as a
  vector. 
  $$
  \textbf{w}_1 =  \begin{bmatrix}
                    w_{1,1},
                    w_{1,2},
                    w_{1,3} ,
                    \dots 
                    w_{1, R}
                  \end{bmatrix}
  $$
  Note that the weight vector is a row vector not a column vector this
  necessary for when we will have to multiply with the input vector. 
</p>

<p>
  Just as before we are simply multiplying the input by the weight. So our new
  vector would be just to multiply each input by the weight on that edge. 
  $$
  \begin{bmatrix}
    w_{1,1} p_1,
    w_{1,2} p_2,
    w_{1,3} p_3,
    \dots
    w_{1,R} p_R
  \end{bmatrix}
  $$
  This is the same as \( \textbf{w}_1 \textbf{p}\). 
</p>

<p>
  The next step is to go through the summation. Summing up the components of this
  vector gives 
  $$ w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R $$
  Then we add in the bias \(b\) which as before is just a single scalar.
  $$ n = w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R + b =
  \textbf{w}_1 \textbf{p} + b$$
</p>

<p>
  Note that \(n=\textbf{w}_1 \textbf{p}+b\) is still just a scalar. We then
  transform the input by the activation function to get the final output of the
  node. 
  $$ a = f(\textbf{w}_1 \textbf{p} + b )$$
</p>

<p>
  We know that the weights of a neuron control how a decision is made by the
  neuron. Different weights will give a neuron different decision properties.
  What if we wanted to work with multiple neurons each having different weights
  at the same time? We could do this by stacking the neurons into a
  <i>layer</i> of neurons where the inputs are feed into each neuron in
  parallel. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/neuron_layer.png'></img>

<p>
  Now the same principle applies as before. For node \(i\) calculate the output
  \(a_i\) through the following formula. Note that we are assuming that all of
  the activation functions are the same which as we will see later is a safe
  assumption to make for this case.
  $$ a_i = f(\textbf{w}_i \textbf{p} + b) $$
  However, we can simplify this and view the weights as a matrix of weights
  represented as follows saying there are \(S\) nodes that the input is being fed
  into.
  $$ \textbf{W} = 
  \begin{bmatrix}
    w_{1,1} & w_{1,2} & w_{1,3} & \dots & w_{1,R} \\
    w_{2,1} & w_{2,2} & w_{2,3} & \dots & w_{2,R} \\
    w_{3,1} & w_{3,2} & w_{3,3} & \dots & w_{3,R} \\
    \dots & \dots & \dots & \dots & \dots \\
    w_{S,1} & w_{S,2} & w_{S,3} & \dots & w_{S,R} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \textbf{w}_1 \\
    \textbf{w}_2 \\
    \textbf{w}_3 \\
    \vdots \\
    \textbf{w}_S \\
  \end{bmatrix}
  $$
  We also no longer have a single bias but now a bias for each neuron and there
  are \(S\) neurons
  $$
  \textbf{b} = \begin{bmatrix}
  b_1,
  b_2,
  b_3,
  \dots
  b_S
  \end{bmatrix}
  $$

  Likewise, we can say there is an output vector \(\textbf{a}\) that can be
  calculated through the following formula.
  $$ \textbf{a} = f(\textbf{W} \textbf{p} + \textbf{b})$$
</p>


<p>
  What happens if we feed the outputs of one layer of neurons into another
  layer of neurons? 
</p>

<img class='center-image' src='/static/img/ml/crash_course/multiple_layers.png'></img>

<p>
  This is where we begin to see the power of neural networks. Each layer of
  neruon works on the abstraction of the previous layer. This allows deeper
  layers to make more complex and higher level decisions. Let's take a concrete
  example. Say you were builidng a neural network that takes as input
  handwritten images. The first layer could detect edges. The second could
  identify the contour the edges form. The third could take these contours and
  identify them with shapes. The fourth and final could take these shapes and
  associate them with numbers. 
</p>

<p>
  Now notate the weight matrix for layer \(i\) to be \(\textbf{W}^{i}\). Note
  that the number of neurons stacked vertically does not have to be the same in
  each layer. The neurons simply connect to all of the next level so it does
  not matter how neurons are in the previous layer for the current layer. The
  example below is a little more concrete of such a network.
</p>

<img class='center-image' src='/static/img/ml/crash_course/network_example.png'></img>

<p>
  We can simply extend the rules used to compute the output of one layer and
  extend it to multiple layers. For instance to compute the output of the above
  network would be the following. 
  $$
  \textbf{a}^3 = f^3 ( \textbf{W}^3 f^2 ( \textbf{W}^2 f^1 (\textbf{W}^1
  \textbf{p} + \textbf{b}^1) + \textbf{b}^2) + \textbf{b}^3 )
  $$
  Notice how the input \(p\) is propagated from the left of the network to the
  right of the network.
</p>

<p>
  Typically you will see neural networks illustrated in the less expressive
  version shown below to save space. In the illustration each neuron is simply
  a node.
</p>

<img class='center-image' src='/static/img/ml/crash_course/nn_illustration.png'></img>

<p>
  That is all for the basics of a neural network building blocks. You should be
  able to see how a neural network produces an output for some input. But what
  are all these transformations doing? The key is choosing the right values for
  the weights and the biases to make the network do interesting things. We can
  do this through having the network <b>learn</b> the weights and biases.
</p>

<p>
  We simply tell the neural network to learn, for some datset \(X,
  Y\), the mapping from \( X \) to \( Y \) and the network will find the
  appropriate weights to do so. 
</p>

<p>
  The neural network is a statistical learner. Given a set of input data \(X\)
  and a set of output data \( Y \) a neural network can learn the mapping from
  \( X \) to \( Y \). 
</p>
