TITLE:Building Blocks of a Neural Network. The Neuron
|
NEXT:
|
PREV:1_neuron_theory
|
Date:Feb 8, 2017
|
META: tmp
|
<p>
First let's consider this basic network shown below. This is known as an
Adaptive Linear Neuron (ADALINE) network.
</p>

<img class='center-image' src='/static/img/ml/crash_course/adaline.png'></img>

<p>
  We can then say the output of this network is:
  $$ a = \textbf{wp} + b $$
  Change our ADALINE network to a more concrete example with a network that
  only has 2 inputs. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/adaline_two_input.png'></img>

<p>
  Now let's use our network to classify generic inputs 
  \(   \begin{bmatrix}
    \textbf{p}_1 \\
    \textbf{p}_2 
  \end{bmatrix} \). We can begin to construct this classifier by saying that
  for any output \( a > 0 \) the input is class one and for any output \( a \le
  0\) the input can be categorized as class two. We can find the decision
  boundary by setting the equation of the network equal to zero. 
  $$ \textbf{wp} + b = 0 $$
  $$ w_1 p_1 + w_2 p_2 + b = 0$$
  $$ w_2 p_2 = - w_1 p_1 - b $$
  $$ p_2 = - \frac{w_1}{w_2} p_1 - \frac{b}{w_2} $$

  It should be clear that this is in the form \( y = mx + b \). Let's graph
  this equation.
</p>

<img class='center-image' src='/static/img/ml/crash_course/adaline_decision.png'></img>

<p>
  All of the area covered in gray is for class one and all the white area is
  class two. This is a basic linear classifier. Think of it like linear
  regression but instead of continuous outputs its output is a category or
  class.
</p>

<p>
  We will now discover an algorithm that can be used to find the line to best
  seperate data points. Take the example below where a line is discovered to
  best seperate the blue and the red data points. We want to algorithmically
  find this line. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/linear_class.png'></img>

<p>
  Let's simplify how we represent this equation by putting all of the
  parameters that are to be adjusted in one term. Note that \( p_1 \) and \(
  p_2 \) cannot be adjusted but are given. Say that \( \textbf{x}^T =
  \begin{bmatrix}
    \textbf{w} \\
    b
  \end{bmatrix} \) and \( \textbf{z} = 
  \begin{bmatrix}
    \textbf{p} \\
    1
  \end{bmatrix} \) notice that the dot product of these two vectors gives the
  same result as before 
  $$ a = \textbf{x}^T \textbf{z} = \textbf{wp} + b $$
  Transposing the \( \textbf{x} \) term is to simply make the product of \(
  \textbf{x} \) and \( \textbf{z} \) possible and to write both of them as
  column vectors. Remember the rules of matrix multiplication.
</p>

<p>
  We can say that the error is the value we <i>expected</i> subtracted by the
  output of the network. Say \( t \) was the value expected and \( a \) was the
  output of the network. The error would then be \( t - a \). A common measure
  for error is the mean square error. This is given by the expected value of
  the squared error. The expected value here is the expected value of a given
  input given consideration of all input/output pairs. Mathematically we can
  write this as
  $$ F(x) = E[e^2] = E[(t-a)^2] = E[(t-\textbf{x}^T \textbf{z})^2] $$
  Expanding this expression then gives the following. Note that we can use the
  linearity of expected value to split the expected value terms apart.
  $$ F(x) = E[t^2 - 2 t \textbf{x}^T \textbf{z} + \textbf{x}^T \textbf{z} \textbf{z}^T \textbf{x} ] $$

  Note that in the last term all of the funky buisness done with the transposes is just to make
  the matrix multiplication work out. \( \textbf{x}^T \textbf{z}
  \textbf{x}^T \textbf{z} \) is not a valid product but \( \textbf{x}^T \textbf{z}
  \textbf{z}^T \textbf{x} \) is. Next apply use the linearity of expected
  value.

  $$ F(x) = E[t^2] - 2\textbf{x}^T E[t\textbf{z}] + \textbf{x}^T
  E[\textbf{z}\textbf{z}^T]\textbf{x} $$

  Let's just simplify the form a little bit by making the following
  substitutions.
  $$ F(\textbf{x}) = c - 2\textbf{x}^T\textbf{h} + \textbf{x}^T\textbf{Rx} $$
  where
  $$ c = E[t^2], \textbf{h} = E[t\textbf{z}], \textbf{R} = E[\textbf{z} \textbf{z}^T] $$
</p>

<p>
  It turns out this is in the form of a quadratic function. Finding the minimum
  point of this quadratic function will minimize the mean square error which is
  our goal. We can do this by just finding the gradient. of the function and
  setting it equal to \( 0 \).

  $$ \nabla F(\textbf{x}) = \nabla ( c - 2\textbf{x}^T\textbf{h} +
  \textbf{x}^T\textbf{Rx} ) = -2\textbf{h} + 2\textbf{Rx} = 0$$

  $$ x = \textbf{A}^{-1} \textbf{h} $$
</p>
