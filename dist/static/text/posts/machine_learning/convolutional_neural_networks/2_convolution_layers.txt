TITLE:Convolutional Layers
|
NEXT:3_pooling_layers
|
PREV:1_motivation
|
Date:Feb 8, 2017
|
META: tmp
|
<h4>Convolution Operator</h4>

<p>
  First of all what does convolution even mean? The convolution is a
  mathematical operation used to extract features from an image. The
  convolution is defined by an image kernel. The image kernel is nothing more
  than a small matrix. For instance, a 3x3 kernel matrix is very common.
  Remember that we can think of an image as a array of numbers. For simplicity
  let's just look at greyscale images at first. 
</p>

<p>
  For instance, say this is our image. (Where we have 1 being black and 0 being
  white).
</p>

<img class='center-image' src='/static/img/ml/cnn/image.png'></img>

<p>
  And this is our 3x3 kernel. 
</p>

<img class='center-image' src='/static/img/ml/cnn/kernel.png'></img>

<p>
  The kernel is applied to the image by sliding it over the image and for each
  position computing the element wise multiplication of the two matrices and
  then summing up the result.
</p>

<img class='center-image' src='/static/img/ml/cnn/kernresult.gif'></img>

<p>
  A different kernel will obviously produce a different output. The kernel can
  be specially selected to perform certain operations on the output image such
  as edge detection, sharpening or blurring. Below is an example of an edge
  detection kernel of \( 
  \begin{bmatrix}
    -1 & -1 & -1 \\
    -1 & 8 & -1 \\
    -1 & -1 & -1
  \end{bmatrix}
  \) applied to an image.
</p>

<img class='center-image' src='/static/img/ml/cnn/edgedetectresult.jpg'></img>

<p>
  Now we can use multiple kernels to extract different features from an image.
  A demonstration of this is shown below. First the red kernel is slid over the
  input image to extract a feature map (just the result of the convolution) and
  next a different kernel colored in green does the same to produce a different
  feature map. 
</p>

<img class='center-image' src='/static/img/ml/cnn/featuremap.gif'></img>

<p>
  A large part of the reason why the convolution operator is such a powerful
  feature detector is that it is space invariant. Since the kernel slides over
  the entire image it does not matter where in the image a given "feature" may
  be, the kernel will find it regardless. Furthermore, the kernel is applied
  uniformly (the values of the kernel do not change as it is slid over the
  image) so it will detect a feature the same in one part of an image as
  another. 
</p>

<p>
  Still confused about the convolution operator? Check out <a
  href='http://setosa.io/ev/image-kernels/'>this incredible demo</a> for an
  interactive way to explore convolution.
</p>

<p>
  The core idea of the CNN is to learn the values of these filters through
  backpropagation and to stack multiple layers of feature detectors (kernels)
  on top of each other for abstracted levels of feature detection. 
</p>

<h4>A Formal Definition of Convolution</h4>

<p>
  To gain a more in depth understanding of what convolution is let's take a
  look at it through a more rigorous definition. Convolution is a mathematical
  operation that has roots in signal processing. Say we are measuring the
  noisy signal \( x(t) \). Since this signal is noisy we can get a more
  accurate measurement by averaging the signal with the last several signals.
  However, more recent measurements are more relevant to the current position
  and we therefore want to have some sort of decaying weight function that
  penalizes measurements that happened a long time ago. Say \( w(a) \) is our
  weight function where \( a \) is the age of the measurement. Now say we
  wanted to get a measurement at time \( t \) taking into account this
  weighting. To do so we would have to apply the weighted average at every
  continuous moment in time before \( t \)
  $$
  s(t) = \int_{0}^{t} x(a) w(t-a) da
  $$
  This operation is the definition of convolution and is often notated as \(
  s(t) = (x*w)(t) \) The convolution function can be thought as the amount of
  overlap of functions \( x \) and \( w \). In the below image the green curve
  is the value of the convolution \( f * g \), the red is \( f \), the blue \(
  g \) and the shaded area is the product \( f(a) g(t - a) \) where \( t \) is
  the x-axis. 
</p>

<img class='center-image' src='/static/img/ml/cnn/convgaus.gif'></img>

<p>
  The first argument to the convolution (in the example the function \(x(t)\))
  is the input to the function and the second (in the example the function \(
  w(t) \) ) is referred to as the kernel. 
</p>

<p>
  This continuous representation of convolution does not work for computers
  that only work with discrete values. We can convert the convolution to its
  discrete counterpart.
  $$
  s(t) = (x*w)(t) = \sum_{a=-\infty}^{\infty} x(a)w(t-a)
  $$
  However, remember that our goal is to apply this to images which have defined
  boundaries so we can constrain these infinite sums to the dimensions of the
  image.
</p>

<p>
  Furthermore, images are two dimensional so we must apply the convolution to
  a two dimensional function. 
  $$
  S(i, j) = (I * K)(i, j) = \sum_{m} \sum_{n} I(m,n) K(i - m, j - n)
  $$
  Get used to the notation of calling \( I \) as the input image and \( K \) as
  the kernel. Furthermore, in the above equation \( n \) and \( m \) would be
  clamped to the dimensions of the image.
</p>

<h4>Sparse Connections</h4>

<p>
  Put the convolution operator on the backburner for a little while because
  first we have to visit the next major breakthrough with convolutional neural
  networks: sparse connectivity. 
</p>

<p>
  In traditional neural networks each neuron is connected to every neuron in
  the next level. If a given layer has \( m \) inputs and \( n \) outputs the
  runtime of the matrix multiplication needed to compute the layer output is \(
  O( m * n) \). The connections of a fully connected layer are shown below. 
</p>

<img class='center-image' src='/static/img/ml/cnn/full_connection.png'></img>

<p>
  However, say we limit the number of connections each node has to \( k \). The
  runtime of such an approach is now \( O(k * n) \) where \( k \) is orders of
  magnitude smaller than \( m \)
</p>

<img class='center-image' src='/static/img/ml/cnn/sparse_connection.png'></img>

<h4>Convolution Layer</h4>

<p>
  The convolution layer is at the core of a CNN. Recall that a standard neural
  network layer takes a vector as input. A CNN takes a 
  <i>volume</i> as input. We will see how this concept makes
  sense soon. The neurons of a convolution layer are likewise arranged as
  volumes. The weights of a convolution layer can be viewed as a set of filters
  (remember from the convolution operator).
</p>

<img class='center-image' src='/static/img/ml/cnn/cnn_vols.jpeg'></img>

<p>
  There are two primary concepts that make the convolution layer possible. The
  first is local connections. Just as explained above this makes only parts of
  the input connected to the neurons. In terms of convolutional layers this
  means that only patches of the input layer are connected to a given filter.
  The spatial extent of the weights connections to the input is called the
  receptive field or the filter size. The connectivity is always full along the
  depth axis. So in summary, connections are local in width and height, while full in
  the depth dimension. 
</p>

<p>
  Say an input volume is 32x32x3, meaning it is a 32x32 RGB image. We say that
  this image has 3 depth slices each of which have an area of 32x32 (where each
  depth slice corresponds to one of the color channels). If we choose a filter
  size of 5x5 in this situation then each neuron in the convolution layer would
  be connected to \( 5*5*3 = 75 \) weights (not including bias).  Note that
  this 5x5 filter size had to be connected to the full extent of the depth. 
</p>

<p>
  As we will see shortly it makes sense to call the incoming weights of the
  neuron a filter. 
</p>

<p>
  So how many neurons in the convolution layer are needed and what local
  patches are these neurons connected to? The hyper parameters of depth, stride and
  zero padding address all of this.
</p>

<p>
  Depth is the the number of neurons looking at the same local region of input.
  This will control the depth of the output volume. As each neuron can be
  viewed as a filter this controls the number of filters in the layer.
  Furthermore, each of these filters will be trained to detect something else
  on the same input region. 
</p>

<p>
  Stride specifies how to slide a filter over the input region. This is what
  connects one filter to the input region. For instance if the stride is 2
  the filter will move 2 pixels at a time. Furthermore, this can be viewed as a
  form of downsampling because higher strides will produce smaller output
  volumes. 
</p>

<p>
  Zero padding is simply padding the input image with 0's. So a zero padding of
  3 would add 3 layers of zero valued pixels around the borders. In practice
  zero padding gives a way to control the output volume without having to
  choose between smaller spatial extent or smaller kernels. 
</p>

<p>
  So the output volume size is dependent on the input volume size \( W \), the
  filter size \( F \) of the neurons and the stride \( S \) and the amount of
  zero padding \( P \). The output volume height and width is given by the following formula. 
  $$
  \frac{W - F + 2P}{S} + 1
  $$
  So for a 32x32 input and a 5x5 filter with a stride of 1 and zero padding of
  0 we would get a 28x28 output. Of course, fractional pixel values do not make
  sense so there are some combinations of \(W, F, P\) and \( S \) that do not work.
</p>

<p>
  The depth of the output layer is controlled by the depth of the neurons. We
  can notate this parameter as \( K \). 
</p>

<p>
  Now say that the filter size is 11, a stride of 4, depth is 96 and no zero
  padding. Say the input volume has a size of 227x227x3. Using the above
  formula we can deduce that the output of the first layer will have dimensions
  of 55x55x96. All 96 neurons along a depth column are connected to the same
  input region of dimension 11x11x3. 
</p>

<p>
  Now in the above example we would have a total of 55*55*96 = 290,400 neurons
  in just the first layer with each neuron having 11*11*3 weights (and 1 bias).
  Together this adds up to over 100 million parameters. This number can be
  reduced through <i>parameter sharing</i>. Parameter sharing makes the
  assumption that it is useful for a filter to calculate the same features
  across the entire region of a depth slice. This is a good assumption to make
  in most cases as remember this neural network will be many layers deep and
  therefore will have multiple layers of abstraction. We can use this
  assumption by making a single depth slice share the same weights and biases.
  So now instead of 55*55*96 neurons there are only 96 neurons. And as before
  each of those 96 neurons is connected to a 11*11*3 volume giving \(
  96*11*11*3 = 34,848 \) parameters (not including biases). 
</p>

<p>
  Now if all the neurons in a single depth slice are using the same weights
  then computing the forward pass of a convolution layer is the same as
  doing the convolution of the neuron's weights and the input volume. This
  is why we refer to the sets of weights for each neuron to be a filter or a
  kernel. 
</p>

<p>
   The same rules that applied to regular neural networks also apply to
   CNNs. The same concepts of backpropagation still apply and the network is
   still composed of numerous layers and a loss function. Let's take a closer
   look at what these filters are computing. Below is an image of 96 filters in
   the first layer of a convolutional neural network used in image detection. 
</p>

<img class='center-image' src='/static/img/ml/cnn/filters.png'></img>

<p>
  Remember by the assumption of parameter sharing, we are assuming that if a
  filter learns that detecting a horizontal edge at some location in the image
  is important it will be important at all locations of the image. This is a
  fair assumption as typically images are invariant to translation. (i.e. if a
  picture of a cat has a cat in the top left hand corner, it is still a picture
  of a cat if the cat is moved to the opposite corner). 
</p>

<p>
  In my opinion the most valuable resource for really getting a solid grasp on
  how the whole process work is the convolution demo made for the Stanford 231n
  course. A quick note before checking out the demo, this demo is for when there are
  two filters of size 3, a stride of 2, and a zero-padding of 1. Note that the
  filter weight matrices are both composed of 3 3x3 matrices because the input
  volume has a depth of 3. I would recommend spending a lot of time studying
  the demo and getting a good grasp on what is happening. <a
  href='https://cs231n.github.io/assets/conv-demo/index.html'>Please find the
  demo here</a>. 
</p>

<p>
  While the convolution layer is certianly the driving innovation behind CNNs
  there is one more layer that is typically used in CNN architectures. Read the
  next section to find out more!
</p>
