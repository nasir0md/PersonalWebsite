TITLE:CNN Applications
|
NEXT:sources
|
PREV:3_pooling_layers
|
Date:Feb 8, 2017
|
META: tmp
|
<h4>Fully Connected Layers</h4>

<p>
  Before getting into the actual architecture of CNNs we first have to know
  that convolutional layers alone cannot make up a neural network that outputs
  a vector (which we need for classification and regression). A CNN outputs a
  3D volume which is a tensor, not a vector. 
</p>

<p>
  The basic architecture of a CNN is to stack successive layers of convolution and max
  pooling. However, after several convolution, max pooling pairs the image has
  been sufficiently processed to a low dimension, meaningful signal. 
  At this point the output of the convolution or max pooling layer
  is <i>flattened</i> and passed through several fully connected layers. These
  fully connected layers are just the basic neural network connections
  presented in the <a href='/blog/machine_learning/neural_networks/introduction'>previous</a> tutorial on neural network basics. Two or three of
  these fully connected layers followed by an output layer is common. 
</p>

<p>
  This fully connected network at the end of the CNN is referred to as the
  decision network. This is because it takes the signal from the convolution layers and
  makes a meaningful decision from it. 
</p>

<h4>Common Architectures</h4>

<p>
  Now let's take a look at actual CNN architectures. The development of CNNs
  have been a driving force behind the development of deep learning. As you
  will see CNNs gradually got deeper and deeper over history. Deeper CNNs are
  more powerful object detectors on high resolution images. This is only a sample of
  the CNN architectures that are out there. New architectures are coming out
  every day. 
</p>

<ul>
  <li>
    <b>LeNet-5</b>: This network is the original CNN. It is often used as a
    basic example for the MNIST dataset which is comprised of 32x32x1 images of
    handwritten digits (grey scale digits). The network architecture is in the following order:
    <ul>
      <li>
        Convolutional layer composed of 6
        5x5 filters. This reduces the input to 28x28x6.
      </li>
      <li>
        Downsampled by maxpooling that has a kernel size of 4 which gives an
        volume size of 14x14x6  
      </li>
      <li>
        Convolution layer with 16 filters for 10x10x16. 
      </li>
      <li>
        Max pooling layer reducing output to 5x5x16
      </li>
      <li>
        Flatten output
      </li>
      <li>
        Fully connected layer of 120 neurons
      </li>
      <li>
        Fully connected layer of 84 neurons
      </li>
      <li>
        Fully connected layer of 10 neurons
      </li>
      <li>
        Softmax function to determine class scores. (This was in the context of
        digit recognition and there are 10 digits)
      </li>
    </ul>
    A diagram of the network can be found below.

    <img class='center-image' src='/static/img/ml/cnn/lenet5.png'></img>
  </li>
  <li>
    <b>AlexNet:</b> This network has a very similar architecture to LeNet-5 so
    I will not go into the exact details. The first 5 layers are convolution
    followed by max pooling then the last 3 layers are fully connected layers.
  </li>
  <li>
    <b>VGGNet:</b>  The network architecture is listed below. Note that C-64
    means convolution layer with 64 filters, P-2 means pooling layer with
    filter size of 2 and FC-4096 means full connected layer with 4096 neurons. 
    Notice that this network follows up a block of two or
    sometimes three convolution layers with a max pooling layer.
    <ul>
      <li>Input: [224x224x3]</li>
      <li>C-64: [224x224x64]</li>
      <li>C-64: [224x224x64]</li>
      <li>P-2: [112x112x64]</li>
      <li>C-128: [112x112x128]</li>
      <li>C-128: [112x112x128]</li>
      <li>P-2: [56x56x128]</li>
      <li>C-256: [56x56x256]</li>
      <li>C-256: [56x56x256]</li>
      <li>C-256: [56x56x256]</li>
      <li>P-2: [28x28x256]</li>
      <li>C-512: [28x28x512]</li>
      <li>C-512: [28x28x512]</li>
      <li>C-512: [28x28x512]</li>
      <li>P-2: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>P-2: [7x7x512]</li>
      <li>Flatten</li>
      <li>FC-4096: [4096]</li>
      <li>FC-4096: [4096]</li>
      <li>FC-1000: [1000]</li>
      <li>Softmax for class probabilities</li>
    </ul>
    Below is a visualization of VGGNet

    <img class='center-image' src='/static/img/ml/cnn/vggnet.jpg'></img>

    <p>
      This network was a step in the direction of very deep neural networks. It
      showed that depth was an important property for the power of neural
      networks. 
    </p>

    <p>
      However, after this network researchers struggled to come up with deep
      networks that could maintain the original features of the images. This
      lead to the innovation of residual neural networks that featured skip
      connections to allow networks to preserve original image features.
    </p>
  </li>

  <li>
    <b>ResNet</b>: This architecture takes advantage of residual or skip
    connections and allows for much deeper neural networks. Typically ResNets of
    56 layers deep are employed. 
  </li>

  <li>
    <b>Inception v3</b>: Google's inception v3 network is a very deep neural
    network that is currently (or as of writing this) state of the art in CNN
    architectures. The network has 25 million trainable parameters and does 5
    billion multiply-adds for a single forward pass through. 

    <img class='center-image' src='/static/img/ml/cnn/inception_v2.png'></img>
  </li>
</ul>

<p>
  Now you might be wondering how people came up with such models. As of
  writing this the experts in the field are largely also unsure as to why deep neural
  networks perform so well. As a result the design process for these new
  networks is typically adding more layers and trying new things with the
  network until something works. This process has been referred to as <a
  href='https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/'>Grad
  Student Descent</a> (a joke on gradient descent that mocks grad students
  iteratively guessing network architectures without reason to desperately get
  a paper published).
</p>

<h4>Applications in Society</h4>

<p>
  Now that we know what the sophisticated models are, we can get to
  where the real fun begins. Convolutional neural networks aren’t just a cool
  toy for CS nerds, or even an academic proof of concept: turns out, they can
  used to help solve problems beyond just the realm of computer science — and
  in many instances, they already have. When organizations have lots of
  spatially-structured data that they want to better understand, machine
  learning is there to lead the way.
</p>

<p>
  Below are some examples of how convolutional neural networks have been, and
  currently are being, applied in the real world. Our hope is that through
  continuing to use these cutting-edge technologies in positive ways, people will
  eventually come to see these technologies not as something to fear, but rather
  as a potential force for good. After going through this lesson, perhaps you,
  too, could think of a new way to use this exciting technology to help better
  the world around us.
</p>

<p>
  How are CNNs being used for social good? (Just to name a few examples)
  <ul>
    <li>
      Medicine: analyzing 3D lung scans to spot lung cancer <a
      href='https://www.kaggle.com/c/data-science-bowl-2017'>(Data Science Bowl 2017)</a>
    </li>
    <li>
      Environmentalism: detecting deforestation from satellite imagery 
      <a href='https://www.kaggle.com/c/planet-understanding-the-amazon-from-space'>(Kaggle Competition)</a>
    </li> 
    <li>
      Transportation: creating safer self-driving cars (NVIDIA: 
      <a href='https://arxiv.org/abs/1604.07316'>Paper</a>, 
      <a href='https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/'>Blog</a>)
    </li>
    <li>
      Wildlife Protection: training drones to patrol for poachers and animals from the sky 
      <a href='http://teamcore.usc.edu/projects/uavs/index.html'>(USC
      Teamcore)</a>
    </li>
  </ul>
</p>

<h4>Further Resources</h4>

<p>
  For further resources please see the following links to youtube videos:
</p>

<ul>
  <li>
    <a href='https://www.youtube.com/watch?v=FmpDIaiMIeA'>How Convolutional Neural Networks Work</a>
  </li>

  <li>
    <a href='https://www.youtube.com/watch?v=py5byOOHZM8&t=768s'>Neural Network
    that Changes Everything</a>
  </li>

  <li>
    <a href='https://www.youtube.com/watch?v=BFdMrDOx_CM&t=307s'>Inside a
    Neural Network</a>
  </li>
</ul>
