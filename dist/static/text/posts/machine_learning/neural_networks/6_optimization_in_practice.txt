TITLE:Optimization in Practice
|
NEXT:7_theory_epilogue
|
PREV:5_regularization
|
Date:Feb 8, 2017
|
META: tmp
|
<h3>Mini-batch Algorithm</h3>
<p>
  First let's review our SGD algorithm shown below. 
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \alpha \nabla J(\textbf{x}_k,
  \mathbf{\theta}(k-1))
  $$
  As this algorithm is <i>stochastic</i> gradient descent it operates one input
  example at a time. This is also referred to as online training. However, this
  is not an accurate representation of the gradient as it is only over a single
  input parameter and is not necessarily reflective of the gradient over the
  entire input space. A more accurate representation of the gradient could be
  given by the following.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \alpha \nabla J(\textbf{x},
  \mathbf{\theta}(k-1))
  $$
  The gradient at each iteration is now being computed across the entire input
  space. This is referred to as batch gradient descent which we will see in a
  second is a confusing name. 
</p>

<p>
  In practice neither of these approaches are desirable. The first does not
  give good enough of an approximation of the gradient, the second is
  computationally infeasible as for each iteration the gradient of the cost
  function for the entire dataset has to be computed. <i>Mini-batch</i> methods
  are the solution to this problem.
</p>

<p>
  In mini-batch training a set of the samples are used to compute the cost
  gradient. The average of these gradients for each sample is then used. This
  approach offers a good trade off between speed and accuracy. The equation
  for this method is given below where \( Q \) is the number of samples in the
  mini-batch and \( \alpha \) is the learning rate as before.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{Q} \sum_{q=1}^{Q}\nabla
  J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$
  Remember that batch gradient descent is over the whole input space while
  mini-batch is just over a subset at a time. 
</p>

<p>
  Of course it would make sense that the samples have to be randomly drawn from
  the input space as sequential samples likely have some correlation. The
  typical procedure is to randomly shuffle the input space and the sample
  sequentially for mini-batch.
</p>

<h3>Initializations</h3>

<p>
  At this point you may be wondering how a neural network is actually
  initialized. So far the learning has been described but the actual initial
  state of the network has not been discussed. 
</p>

<p>
  You may think that how a network is initialized does not necessarily matter.
  After all the network should eventually converge to the correct parameters
  right? Unfortunately this is not the case with neural networks, the
  initialization matters greatly. Initializing to small random weights
  typically works. However, the standard for weight initialization many
  consider to be the normalized initialization method. 
</p>

<p>
  Using this method weights are randomly drawn from the following uniform
  distribution.
  $$
  \textbf{W} \sim U \left( -\frac{6}{\sqrt{m+n}}, \frac{6}{m+n} \right)
  $$
  Where \( m \) is the number of inputs into the layer and \( n \) is the 
  number of outputs from the layer. 
</p>

<p>
  As for the biases, typically just assigning them to a value of 0 works. 
</p>

<h3>Challenges in Optimization</h3>

<p>
  In classic math optimization functions we optimize some function \( f \). Now
  the same is true here where we are optimizing the loss function. However,
  keep in mind this loss function is not the same as the objective function
  which is how the model is performing on the actual inputs. This means that
  the gradient of the loss function is just an approximation of the true
  gradient of the objective function. 
</p>

<p>
  Another concern are local minima. Any deep neural network is guaranteed to
  have a very large number of local minima. Take a look at the below
  surface. This surface has two minima, a local and a maximum. If you look at
  the below contour map you can see that the algorithm converges to the local
  minimum instead of the global maximum. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/optimization_surface_minima.png'></img>
  
<p>
  How can we stop our neural network
  from converging to local minima? Well local minima would be a concern if the
  cost function evaluated at the local minima was far greater than the cost
  function evaluated at the global minima. It turns out that this difference is
  negligible. Most of the time, simply finding any minima is sufficient in the
  case of deep neural networks. 
</p>
  
<p>
  Another issue is saddle points, plateaus or valleys. In practice neural
  networks can escape valleys or saddle points. However, they can still pose a
  serious threat to neural networks as they can have cost functions much
  greater than the global minimum. Even more dangerous are flat regions. Small
  weights are chosen in part to avoid these flat regions in the performance
  surface. 
</p>

<p>
  In general more flat areas are problematic for the rate of convergence. It
  takes a lot of iterations for the algorithm to get over more flat regions.
  The first thought may be to increase the learning rate of the algorithm but
  too high of a learning rate will result in divergence at steeper areas of the
  performance surface. When this algorithm with a high learning rate goes
  across something like a valley it will oscillate out of control and diverge.
  An example of this is shown below. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/momentum.png'></img>

<p>
  At this point it should be clear that several modifications to
  backpropagation need to be made to allow solve this oscillation problem and
  to fix the learning rate issue.
</p>

<h3>Momentum</h3>

<p>
  For this concept it is useful to think of the progress of the algorithm
  as a point traveling over the performance surface. Momentum in neural
  networks is very much like momentum in physics. And since our 'particle'
  traveling the performance surface has unit mass, momentum is just the
  velocity. The equation of backprop including momentum is given by the
  following. 

  $$
  \textbf{v}(k) = \lambda \textbf{v}(k-1) - \alpha \nabla J(\textbf{x}, \mathbf{\theta}(k-1))
  $$
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) + \textbf{v}(k)
  $$
  The effect of applying this can be seen in the image below. Momentum dampens
  the oscillations and tends to make the trajectory continue in the same
  direction. Values of \( \lambda \) closer to 1 give the trajectory more momentum.
  Keep in mind \( \lambda \) itself is not momentum and is more like a force of
  friction for the particles trajectory. Typical values for \( \lambda \) are 0.5,
  0.9, 0.95 and 0.99.
</p>

<img class='center-image' src='/static/img/ml/crash_course/momentum_working.png'></img>

<p>
  Nesterov momentum is an improvement on the standard momentum algorithm. With
  Nesterov momentum the gradient of the cost function is considered after the
  momentum has been applied to the network parameters at that iteration. So now
  we have:
  $$
  \textbf{v}(k) = \lambda \textbf{v}(k-1) - \alpha \nabla J(\textbf{x},
  \mathbf{\theta}(k-1) + \lambda \textbf{v}(k-1))
  $$
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) + \textbf{v}(k)
  $$
  In general, Nesterov momentum outperforms standard momentum. 
</p>

<h3>Adaptive Learning Rates</h3>

<p>
  One of the most difficult hyperparameters to adjust in neural networks is the
  learning rate. Take a look at the image below to see the effect of learning
  different learning rates on the minimization of the loss function.
</p>

<img class='center-image' src='/static/img/ml/crash_course/learningrates.jpeg'></img>

<p>
  As from above we know that the trajectory of the algorithm over flat sections
  of the performance surface can be very slow. It would be nice if the
  algorithm could have a fast learning rate over these sections but a slow
  learning rate over steeper and more sensitive sections. Furthermore, the
  direction of the trajectory is more sensitive in some directions as opposed
  to others. The following algorithms will address all of these issues with
  adaptive learning rates.
</p>

<h3>AdaGrad</h3>

<p>
  The Adaptive Gradient algorithm (AdaGrad) adjusts the learning rate of each
  network parameter according to the history of the gradient with respect to
  that network parameter. This is an inverse relationship so if a given network
  parameter has had large gradients throughout the past the learning rate will
  scale down significantly. 
</p>

<p>
  Whereas before there was just one global learning rate, there is now a per
  parameter learning rate. We call the vector \( \textbf{r} \) to be the
  accumulation of the parameters past gradient squared. We initialize this term
  to zero.
  $$
  \textbf{r} = 0
  $$

  Next we compute the gradient as normal 
  $$
  \textbf{g} = \frac{1}{Q} \sum_{q=1}^{Q}\nabla J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$

  And then accumulate this gradient in \( r \) to represent the history of the
  gradient. 

  $$
  \textbf{r} = \textbf{r} + \textbf{g}^2
  $$

  And finally we compute the parameter update
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{\delta +
  \sqrt{\textbf{r}}}
  \odot g
  $$

  Where \( \alpha \) is the global learning rate \( \delta \) is an extremely
  small constant ( \( 10^{-7} \) ). Notice that a element wise vector
  multiplication is being performed (by the \( \odot \) operator). Remember that each element of the gradient
  represents the partial derivative of the function with respect to a given
  parameter. The element wise multiplication will then scale the gradient with
  respect to a given parameter appropriately. The global learning rate is not a
  problem to choose and normally works as just 0.01.
</p>

<p>
  Clearly the problem with this algorithm is that it considers the whole
  sum of the squared gradient since the beginning of training. This results in
  the learning rate decreasing too much too early. 
</p>

<h3>RMSProp</h3>

<p>
  RMSProp is regarded as the goto optimization algorithm for deep neural
  networks. It is similar to AdaGrad but now there is a decay over the
  accumulation of the gradient squared so the algorithm "forgets" gradients far
  in the past. 
</p>

<p>
  As normal, compute the gradient. 
  $$
  \textbf{g} = \frac{1}{Q} \sum_{q=1}^{Q}\nabla J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$

  Now, this is where the algorithm changes with the introduction of the decay
  term \( \rho \).

  $$
  \textbf{r} = \rho \textbf{r} + (1 - \rho) \textbf{g}^2
  $$

  And the parameter update is the same.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{\delta +
  \sqrt{\textbf{r}}}
  \odot g
  $$
</p>

<h3>Second Order Algorithms</h3>

<p>
  Second order algorithms make use of the second derivative to "jump" to the
  critical points of the cost function. Further discussion of these algorithms
  is outside the scope of this tutorial. However, these algorithms do not work
  very well in practice. First of all it is computationally infeasible to
  compute the second order derivatives. Second of all, for a complex
  performance surface with many critical points it is very likely the second
  order method would go in the completely wrong direction. Overall, gradient
  descent first order methods have been shown to work better and perform
  better so I would not worry about knowing what second order algorithms are
  all about. But know that they exist and are an active area of research. 
</p>

<h3>The Unstable Gradient Problem</h3>

<p>
  It turns out deeper neural networks can be a lot more powerful than their
  shallow counterparts. Despite that fact that a two layer network can model
  any function it actually takes exponentially more neurons to do so than
  builder deeper layers. These deeper layers of neurons add more layers of
  abstraction for the network to work with. Deep neural networks are vital to
  visual recognition problems. Modern deep neural networks built for visual
  recognition are hundreds of layers deep. 
</p>

<p>
  However, you may take what you have learned so far and try to build a deep
  neural network. However, to your surprise you may see that adding more layers
  does not seem to help and even reduces the accuracy. Why is this the case?
</p>

<p>
  The answer is in unstable gradients. This problem plagued deep learning up
  until 2012 and is responsible for much of the deep learning boom. The cause
  of the unstable gradient problem can be formulated as different layers in the
  neural network having vastly different learning rates. And this problem only
  gets worse with the more layers that are added. The vanishing gradient
  problem is that earlier layers are learning slower than later layers. The
  exploding gradient problem is the opposite. Both of these issues deal with
  how the sensitivities are backpropagated through the network. 
</p>

<p>
  Let's recall the equation for backprogating the sensitivities.
  $$
  \textbf{s}^{m} = 
  \textbf{D}^m (\textbf{n}^{m}) \left( \textbf{W}^{m + 1} \right)^{T}
  \textbf{s}^{m+1}
  $$
  
  Now let's say the network has 5 layers. Let's compute the various
  sensitivities recursively through the network.

  $$
  \textbf{s}^5 = \frac{\partial J}{\partial \textbf{n}^5}
  $$

  $$
  \textbf{s}^4 = d^4(\textbf{n}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{n}^5}
  $$

  $$
  \textbf{s}^3 = d^3(\textbf{n}^3)(\textbf{W}^{4})^T d^4(\textbf{n}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{n}^5}
  $$

  $$
  \textbf{s}^2 = d^2(\textbf{n}^2)(\textbf{W}^{3})^T d^3(\textbf{n}^3)(\textbf{W}^{4})^T d^4(\textbf{n}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{n}^5}
  $$

  $$
  \textbf{s}^1 = d^1(\textbf{n}^1)(\textbf{W}^{2})^T d^2(\textbf{n}^2)(\textbf{W}^{3})^T d^3(\textbf{n}^3)(\textbf{W}^{4})^T d^4(\textbf{n}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{n}^5}
  $$

  The term for \( \textbf{s}^1 \) is massive, and this is only for a five layer
  deep network. Imagine what it would be for a 100 layer deep network! The
  important take away is that all of the terms are being multiplied together. 
</p>  

<p>
  For a while the sigmoid function was believed to be a powerful activation
  function. Below is an image of the sigmoid function and its derivative. 
</p>

<img class='center-image' src='/static/img/ml/crash_course/sigmoid.png'></img>

<p>
  Say we were using the sigmoid function for our five layer neural network.
  That would mean that \(\textbf{D}^1,\textbf{D}^2,\textbf{D}^3,
  \textbf{D}^4,\textbf{D}^5\) are all the derivative of the sigmoid function shown in
  red. What is the maximum value of that function? It's around 0.25. What
  types of values are we starting with for the weights? Small random values.
  The key here is that the values start small. The
  vanishing gradient problem should now start becoming clear. Because of the
  chain rule we are recursively multiplying by terms less far less than one
  causing the sensitivities to shrink and shrink going backwards in the
  network. 
</p>

<p>
  With this many so multiplication terms it would be something of a magical balancing act
  to manage all the terms so that the overall expression does not explode or
  shrink significantly.
</p>

<p>
  How do we fix this problem? The answer is actually pretty simple. Just use
  the ReLU activation function instead of the sigmoid activation function. The
  ReLU function and its derivative are shown below.
</p>

<img class='center-image' src='/static/img/ml/crash_course/relu.png'></img>

<p>
  As you can see derivative is either 0 or 1 which alleviates the unstable
  gradient problem. This function is also much easier to compute. 
</p>
