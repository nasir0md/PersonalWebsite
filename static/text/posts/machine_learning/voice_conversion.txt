TITLE:Voice Conversion
|
NEXT:
|
PREV:
|
Date:Nov 28, 2017
|
META: Voice conversion, voice style transfer
|
<h4>How well can we convert speech between people's voices?</h4>

<h5>By: Andrew Szot, Arshdeep Singh, Md Nasir, Sriram Somasundaram </h5>

<h3>Introduction</h3>

<p>
  Recently, there have been many results in the image domain with the
  introduction of deeper convolutional models that encode higher dimensional and
  interpretable features (neural style) [<a
  href='https://arxiv.org/pdf/1705.04058.pdf'>1</a>].
  In contrast, the audio domain is less studied with a lack of deep learning
  models catering to audio signals. For a while, traditional audio processing
  techniques remained dominant over deep learning approaches in terms of audio
  classification and detection. Specifically, generative models in audio were
  just until recently dominated by traditional audio processing techniques.
  Rather than using a deep learning model, many of the generative models would
  piece together the correct speech fragments stored in a large database, while
  some others use traditional features fed into a deep learning architecture.
  End-to-end deep learning has been a rarity in speech applications due to the
  fundamental characteristics of temporal structure of speech which contains  so
  many different informations crammed together--speaker characteristics (voice),
  linguistic content (language),  paralinguistics (emotion and behavior). Several
  new generative audio deep learning models such as Tacotron
  [<a href='https://arxiv.org/pdf/1703.10135.pdf'>2</a>] and Wavenet
  [<a href='https://arxiv.org/pdf/1609.03499.pdf'>3</a>] have brought change to this situation. A
  good amount of research has already been done into how these models can be used
  for text to speech (TTS), speech generation, or speech classification. 
</p>

<p>
  Our initial motivation was to see whether a good (our criteria below) latent
  space model for raw audio could be generated and used to do audio style
  transfer, drawing inspiration from the neural style algorithm. Neural (image)
  style transfer is a famous application of convolutional networks where style
  information is encoded in shallow layer activations and content information
  is encoded at deeper layers that have global context. The algorithm takes the
  correlations of style filter activations into a Gram matrix to discard
  spatial information past local textures. 
</p>

<video autoplay loop>
  <source src='/static/img/style_transfer/picasso-periods.mp4' type='video/mp4'>
</video>

<p class='pic-cap'>Audio style transfer being applied to images</p>

<p>
  We were curious if this same idea could be applied to the audio domain.
  Specifically, we choose to pursue a subset of this problem, voice conversion. 
</p>

<p>
  Voice conversion is taking the voice of one speaker, equivalent to the
  “style” in image style transfer, and using that voice to say the speech
  content from another speaker, equivalent to the “content” in image style
  transfer. We choose to focus on voice transfer because it was a well defined
  but relatively unexplored problem. There have been investigations in style
  transfer with combining the two sounds of instruments as seen with Google’s
  NSynth [<a href='https://magenta.tensorflow.org/nsynth'>4</a>]. We thought that voice style
  transfer would be easier to evaluate, and voice conversion is a more
  challenging problem as voice is a harder feature to encode in a model’s
  latent space. This is an interesting problem to tackle both from a technical
  and social perspective and could have applications in music industry, legal
  affairs, robotics, automated talking devices, and more. Our approach was to
  be able to perform voice transfer on non-parallel data. 
</p>

<img class='center-image' src='/static/img/ml/voice_conversion/voice-transfer.png'></img>

<p>
  This means that the style voice could be saying something completely
  different than the content voice. We hoped to design a model that could learn
  a latent space encoding higher dimensional information in audio and would be
  able to generalize the style voice without necessarily being trained on all
  possible phonemes (smallest unit of speech) that could occur in the content
  voice.
</p>

<img class='center-image' src='/static/img/ml/voice_conversion/latent_face.png'></img>
<p class='pic-cap'> Linear Interpolation in latent space for faces [<a href='
https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df'>5</a>]</p>

<p>
  Just as linear interpolation in the latent space for facial recognition
  encodes changes in facial structure, movement in an audio latent space should
  encode an audible changes in the higher dimensional features of audio
  (phones, style). 
</p>

<p>
  Overall, this problem is important to address both on an application and
  theory level. On an application level voice transfer would enable
  voice-assistants of all different styles. You could even create a voice
  assistant that had your own voice. Even though models such as Tacotron
  already exist, our approach to voice style transfer would reduce the amount
  of training time needed as the model does not explicitly need to pick up on
  every annunciation of each phoneme as it can generalize. On a theory level
  voice style transfer would give a lot more insight into the power of
  embeddings and how to find them for data. Working with raw audio is far more
  challenging than images. The length of the audio is variable and the
  structures in the raw audio are far more abstract. Learning an embedding for
  speaker style would be a significant advancement in learning embeddings in
  general. 
</p>

<p>
  There are currently not many approaches to this problem, and existing
  approaches can be broken into three categories: spectrogram processing,
  supervision to extract content and apply style, and models with an
  unsupervised latent space. Each of these approaches has encountered
  significant problems and fails to produce any good sounding results. Most of
  the results sound very robotic and do not resemble the original style at all
  or simply turn out to be white noise. Another issue with the current
  approaches is that the approaches that do get decent results are using a lot
  of supervision and use complex datasets. For instance, an approach that gets
  decent results requires a lot of data for each speaker and also requires the
  target phonemes for each sample. The motivation in our approach was to
  provide a completely unsupervised method that could work on non-parallel
  speaker data. 
</p>

<h3>Approach</h3>

<p>We divided our efforts into 4 approaches as a result of our progression through the field:</p>
<ul>
  <li>
    Spectrogram approaches. The approaches in the frequency domain of audio
    (spectrograms) usually involve existing image style transfer techniques on
    spectrogram images. However, those techniques cannot extract local
    information regarding phonemes for speech and are better at recognizing
    global recurring frequencies in music. To try out a model using spectrogram
    data, we took on the novel task of training a CycleGAN between collections of
    male and female audio as our baseline.
  </li>
  <li>
    Supervised latent space / Unconditional generator. After talking to our
    mentors, they suggested that some degree of supervision for text or
    phonemes might be useful to try out in the latent space. As a result we
    tried as 2-step model that included a supervised Speech recognition encoder
    and a Tacotron decoder for speech synthesis. However, the text to speech
    module for decoding was not conditioned on speaker and had to train on lots
    of audio from a certain style speaker (unconditional generator).
  </li>

  <li>
    Supervised latent space / Speaker conditioned generator. Based on our
    previous approach, we wanted the quality of results that came with
    supervision yet wanted to move closer to our original goal which was to be
    able to extract information from only a few samples of the style speaker.
    As an intermediate step, we tried a model with a generator that was
    conditioned on speakers.
  </li>

  <li>
    Unsupervised latent space / Speaker Conditioned generator. Finally, we
    attempted to newly implement ideas in a fresh Deepmind paper in the audio
    domain with an unsupervised latent space. We implemented Auto Encoder
    variants (VQ-VAE, full Wavenet autoencoder).
  </li>
</ul>

<h3>Data</h3>

<p>
  We trained and evaluated almost all of our models on the CSTR VCTK Corpus
  [<a href='http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html'>6</a>]. This
  dataset contains speech data for 109 English speakers saying 400 sentences
  each. Utterances are saved as WAV files and are 4 to 10 seconds long. While
  data is text annotated and parallel, we didn’t use these features in our
  unsupervised models. Furthermore, we never performed frame alignment to
  achieve truly parallel data. 
</p>

<p>
  We used the TIMIT [<a href='https://catalog.ldc.upenn.edu/ldc93s1'>7</a>] dataset for
  supervised Speech Recognition encoder + Tacotron decoder that includes 630
  speakers each reading 10 sentences. The dataset contains time-aligned
  phonetic and word translations that are useful for supervising the encoder.
</p>

<p>
  We also used the Arctic dataset for many audio samples from a single speaker
  to evaluate the supervised model.
</p>

<h3>Baseline: CycleGAN</h3>

<p>
  Our first approach tackled the problem from the frequency-domain
  representations of raw audio. There has been previous work in applying the
  neural style algorithm (for images) onto the spectrograms of content and style
  audio
  [<a href='https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/'>8</a>]. 
</p>


<hr />
<br /> 
<audio controls>
  <source src="/static/img/ml/voice_conversion/imperial.mp3" type="audio/mpeg">
</audio>
<p class='pic-cap'>Content</p>

<audio controls>
  <source src="/static/img/ml/voice_conversion/usa.mp3" type="audio/mpeg">
</audio>
<p class='pic-cap'>Style</p>

<audio controls>
  <source src="/static/img/ml/voice_conversion/imperial_usa.mp3" type="audio/mpeg">
</audio>
<p class='pic-cap'>Generated</p>

<hr />

<p>
  Results seem to be a mash of the content and style audio and speech style
  transfer (voice conversion) will not work as easily due to the absence of
  identifiable and common frequencies across the audio sample. In general, a
  style algorithm relying on the Gram matrix of the spectrogram of the style
  audio will lose time information and just involve correlations between
  frequencies that are drawn from a small context. For voice conversion, we
  care about local statistics rather than increasing consistent frequencies.
  Furthermore, only operating on one sample for style does not provide a very
  good description of the overall style for that speaker and would
  theoretically only work best in the parallel audio case. 
</p>

<p>
  As inspiration we saw how CycleGAN [<a href='https://arxiv.org/pdf/1703.10593.pdf'>9</a>] had
  been applied to the image domain and wanted to apply the same to the audio
  domain. For our setup we had set \( X \) contain male voice samples and set
  \( Y \)
  contain female voices samples. We figured that this approach was a trivial
  enough application of an existing model to be considered a decent baseline.
  The architecture for CycleGAN is shown below. 
</p>

<img class='center-image' src='/static/img/ml/voice_conversion/cyclegan.png'></img>

<p>
  We split the VCTK dataset into male and female audio collections and applied
  a short-time fourier transform (used
  Librosa[<a href='https://github.com/librosa/librosa'>10</a>], an open-source Python library
  for a lot of audio manipulations) and padded to form a 3-channel image of 256x256. We trained a CycleGAN in Tensorflow
  [<a
  href='https://github.com/ASzot/voice-conversion/tree/master/cycle-gan'>https://github.com/ASzot/voice-conversion/tree/master/cycle-gan</a>].
  The core concept of CycleGAN is performing a translation between sample \( A
  \) from domain
  \( X \) to sample \( B \) in domain \( Y \). A key concept of CycleGAN is that this translation
  is constrained by the fact that the the inverse mapping must undo the
  translation. This is referred to as cycle consistency. A big advantage this
  provided us in our project was that this model was no longer constrained to
  parallel data and could learn to generalize the transformation across many
  speakers each saying multiple utterances. 
</p>

<p>
  First there is a image-to-image translator \( G \) that takes an image from the X
  domain and transforms it to the \( Y \) domain. This generator consists of 3
  convolutional layers followed by 9 residual convolutional layers for an
  encoder then 2 deconvolutions followed by a convolution to get the final
  image. We then define an adversarial loss at this stage. \( G \) is generating
  images from an image in \( X \) to look similar to other images in \( Y \)
  while \( D_Y \)
  tries to discriminate real images in \( Y \) from fake images generated by the
  generator G. This discriminator was 5 conv layers followed by 3 fully
  connected layers. This adversarial loss is defined as
</p>

$$ L(G, D_Y, X, Y) = \textbf{E}(\log(D_Y(y))) + \mathbb{E}(\log(1 - D_Y(G(x))) $$

<p>
  And of course we want the same loss to be defined for the other direction
  from \( Y \) to \( X \) using generator \( F \) and discriminator \( D_X \). Next we want to
  incorporate the Cycle Consistency loss. This loss will simply be defined as
  the difference between the twice transformed sample \( x \), \( F(G(x)) \) and the
  original sample \( x \). The same will be defined the other way with the difference
  between \( G(F(y)) \) and \( y \). This gives an overall loss of:
</p>

$$ L(G, F, D_X, D_Y) =  \mathbb{E}(\log(D_Y(y))) + \mathbb{E}(\log(1 - D_Y(G(x)))) 

\nonumber \\
+ \mathbb{E}(\log D_X(x)) + \mathbb{E}(\log(1 - D_X(G(y)))) +

\nonumber \\
+ \lambda(\mathbb{E}(\lVert F(G(x)) - x \rVert _1) + \mathbb{E}(\lVert G(F(y)) -
y \rVert _1)) $$ 

<p>
  Where lambda controls the importance of the cycle consistency loss. In the
  inference stage of the model we remove the extra padding inserted before and
  use the Giffin Lim algorithm as an audio reconstruction technique. 
</p>

<p>
  While this model has been used extensively for image processing tasks, as far
  as we know, this is the first time the model has been applied to the audio
  domain. After training the model for almost 2 days we got some results.
  Something interesting we noticed that later iterations of the model produced
  noisy audio samples. Below the results are shown:
</p>

<hr />
<br /> 
<audio controls>
  <source src="/static/img/ml/voice_conversion/cyclegan_100k_content.wav" type="audio/wav">
</audio>
<p class='pic-cap'>Content</p>

<audio controls>
  <source src="/static/img/ml/voice_conversion/cyclegan_100k_style.wav" type="audio/wav">
</audio>
<p class='pic-cap'>Style</p>

<audio controls>
  <source
  src="/static/img/ml/voice_conversion/cyclegan_40k_combined_f_to_m.wav" type="audio/wav">
</audio>
<p class='pic-cap'>40k iterations female to male conversion</p>

<audio controls>
  <source
  src="/static/img/ml/voice_conversion/cyclegan_40k_combined_m_to_f.wav" type="audio/wav">
</audio>
<p class='pic-cap'>40k iterations male to female conversion</p>

<audio controls>
  <source
  src="/static/img/ml/voice_conversion/cyclegan_100k_combined_m_to_f.wav" type="audio/wav">
</audio>
<p class='pic-cap'>100k iterations male to female conversion</p>

<hr />

<p>
  Despite the results sounding somewhat robotic they provided a reasonable
  baseline. Furthermore, this is a significant improvement over any other
  approach relying solely on spectrograms that we have seen so far. Generally,
  we were surprised by how good the results turned out. We adapted the image
  processing CycleGAN model and applied it to two large collections with a lot
  of variability. Results are grainy as there is now unifying speech content
  however, there is a perceivable pitch change between genders.
</p>

<p>
  Our main takeaways were the following:
</p>

<ul>
  <li>
    It is hard to train between collections of speech with so much internal
    phonetic variety and only pitch in common
  </li>

  <li>
    Working in the frequency domain can be good for when you are looking at
    cyclic behavior in signals and not so much for speech. We needed to move
    forward from our baseline handling the full scope of the audio data or at
    least use preprocessing that resulted in features with a time dimension.
    Using a standard image processing pipeline was expectedly not working
    great. It was time to turn to speech specific networks. 
  </li>
</ul>

<h3>Sources</h3>
<ol>
  <li><a href='https://arxiv.org/pdf/1705.04058.pdf'>https://arxiv.org/pdf/1705.04058.pdf</a></li>
  <li><a href='https://arxiv.org/pdf/1703.10135.pdf'>https://arxiv.org/pdf/1703.10135.pdf</a></li>
  <li><a href='https://arxiv.org/pdf/1609.03499.pdf'>https://arxiv.org/pdf/1609.03499.pdf</a></li>
  <li><a href='https://magenta.tensorflow.org/nsynth'>https://magenta.tensorflow.org/nsynth</a></li>
  <li><a href='https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df'>https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></li>
  <li><a href='http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html'>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</a></li>
  <li><a href='https://catalog.ldc.upenn.edu/ldc93s1'>https://catalog.ldc.upenn.edu/ldc93s1</a></li>
  <li><a href='https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/'>https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/</a></li>
  <li><a href='https://arxiv.org/pdf/1703.10593.pdf'>https://arxiv.org/pdf/1703.10593.pdf</a></li>
  <li><a href='https://github.com/librosa/librosa'>https://github.com/librosa/librosa</a></li>
</ol>
